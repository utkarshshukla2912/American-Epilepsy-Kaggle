{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy\n",
    "import scipy.io as si\n",
    "from scipy import stats, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import glob, os\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Multiple Transformation\n",
    "Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ApplyManyTransform(object):\n",
    "    def apply(self, datas, meta):\n",
    "        if datas.ndim >= 3:\n",
    "            out = []\n",
    "            for d in datas:\n",
    "                out.append(self.apply_one(d, meta))\n",
    "\n",
    "            return to_np_array(out)\n",
    "        else:\n",
    "            return self.apply_one(datas, meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating  Windows\n",
    " Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Slice:\n",
    "    \"\"\"\n",
    "    Take a slice of the data on the last axis.\n",
    "    e.g. Slice(1, 48) works like a normal python slice, that is 1-47 will be taken\n",
    "    \"\"\"\n",
    "    def __init__(self, start, end=None):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"slice%d%s\" % (self.start, '-%d' % self.end if self.end is not None else '')\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        s = [slice(None),] * data.ndim\n",
    "        s[-1] = slice(self.start, self.end)\n",
    "        return data[s[0]]\n",
    "\n",
    "class Windower:\n",
    "    \"\"\"\n",
    "    Breaks the time-series data into N second segments, for example 60s windows\n",
    "    will create 10 windows given a 600s segment. The output is the reshaped data\n",
    "    e.g. (600, 120000) -> (600, 10, 12000)\n",
    "    \"\"\"\n",
    "    def __init__(self, window_secs):\n",
    "        self.window_secs = window_secs\n",
    "        self.name = 'w-%ds' % window_secs if window_secs is not None else 'w-whole'\n",
    "\n",
    "    def get_name(self):\n",
    "        return self.name\n",
    "\n",
    "    def apply(self, X, meta=None):\n",
    "        if self.window_secs is None:\n",
    "            return X.reshape([1] + list(X.shape))\n",
    "\n",
    "        num_windows = 600 / self.window_secs\n",
    "        samples_per_window = self.window_secs * 399\n",
    "        samples_used = num_windows * samples_per_window\n",
    "        samples_dropped = X.shape[-1] - samples_used\n",
    "        X = Slice(samples_dropped).apply(X)\n",
    "        out = np.split(X, num_windows, axis=X.ndim-1)\n",
    "        out = np.asarray(out)\n",
    "        return out\n",
    "\n",
    "#Windower(100).apply(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def upper_right_triangle(matrix):\n",
    "    accum = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(i+1, matrix.shape[1]):\n",
    "            accum.append(matrix[i, j])\n",
    "\n",
    "    return np.asarray(accum)\n",
    "\n",
    "class Eigenvalues(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Take eigenvalues of a matrix, and sort them by magnitude in order to\n",
    "    make them useful as features (as they have no inherent order).\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return 'eigen'\n",
    "\n",
    "    def apply_one(self, data, meta=None):\n",
    "        w, v = np.linalg.eig(data)\n",
    "        w = np.absolute(w)\n",
    "        w.sort()\n",
    "        return w\n",
    "    \n",
    "class CorrelationMatrix(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Calculate correlation coefficients matrix across all EEG channels.\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return 'corr-mat'\n",
    "\n",
    "    def apply_one(self, data, meta=None):\n",
    "        return np.corrcoef(data)\n",
    "    \n",
    "class UnitScaleFeat:\n",
    "    \"\"\"\n",
    "    Scale across the first axis, i.e. scale each feature.\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return 'unit-scale-feat'\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        return preprocessing.scale(data.astype(np.float64), axis=0)\n",
    "\n",
    "\n",
    "class UnitScale:\n",
    "    \"\"\"\n",
    "    Scale across the last axis.\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return 'unit-scale'\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        return preprocessing.scale(data, axis=data.ndim-1)\n",
    "\n",
    "\n",
    "class Correlation(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Correlation in the time domain. Calculate correlation co-efficients\n",
    "    followed by calculating eigenvalues on the correlation co-efficients matrix.\n",
    "\n",
    "    The output features are (upper_right_diagonal(correlation_coefficients), eigenvalues)\n",
    "\n",
    "    Features can be selected/omitted using the constructor arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale_option, with_corr=True, with_eigen=True):\n",
    "        self.scale_option = scale_option\n",
    "        self.with_corr = with_corr\n",
    "        self.with_eigen = with_eigen\n",
    "        assert scale_option in ('us', 'usf', 'none')\n",
    "        assert with_corr or with_eigen\n",
    "\n",
    "    def get_name(self):\n",
    "        selections = []\n",
    "        if self.scale_option != 'none':\n",
    "            selections.append(self.scale_option)\n",
    "        if not self.with_corr:\n",
    "            selections.append('nocorr')\n",
    "        if not self.with_eigen:\n",
    "            selections.append('noeig')\n",
    "        if len(selections) > 0:\n",
    "            selection_str = '-' + '-'.join(selections)\n",
    "        else:\n",
    "            selection_str = ''\n",
    "        return 'corr%s' % (selection_str)\n",
    "\n",
    "    def apply_one(self, data, meta=None):\n",
    "        data1 = data\n",
    "        if self.scale_option == 'usf':\n",
    "            data1 = UnitScaleFeat().apply(data1)\n",
    "        elif self.scale_option == 'us':\n",
    "            data1 = UnitScale().apply(data1)\n",
    "\n",
    "        data1 = CorrelationMatrix().apply_one(data1)\n",
    "\n",
    "        # patch nans\n",
    "        data1[np.where(np.isnan(data1))] = -2\n",
    "\n",
    "        if self.with_eigen:\n",
    "            w = Eigenvalues().apply_one(data1)\n",
    "\n",
    "        out = []\n",
    "        if self.with_corr:\n",
    "            data1 = upper_right_triangle(data1)\n",
    "            out.append(data1)\n",
    "        if self.with_eigen:\n",
    "            out.append(w)\n",
    "\n",
    "        for d in out:\n",
    "            assert d.ndim == 1\n",
    "\n",
    "        return np.concatenate(out, axis=0)\n",
    "    \n",
    "#Correlation('usf').apply_one(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FreqCorrelation(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Correlation in the frequency domain. First take FFT with (start, end) slice options,\n",
    "    then calculate correlation co-efficients on the FFT output, followed by calculating\n",
    "    eigenvalues on the correlation co-efficients matrix.\n",
    "\n",
    "    The output features are (fft, upper_right_diagonal(correlation_coefficients), eigenvalues)\n",
    "\n",
    "    Features can be selected/omitted using the constructor arguments.\n",
    "    \"\"\"\n",
    "    def __init__(self, start_hz, end_hz, option, use_phase=False, with_fft=False, with_corr=True, with_eigen=True):\n",
    "        self.start_hz = start_hz\n",
    "        self.end_hz = end_hz\n",
    "        self.option = option\n",
    "        self.with_fft = with_fft\n",
    "        self.with_corr = with_corr\n",
    "        self.with_eigen = with_eigen\n",
    "        self.use_phase = use_phase\n",
    "        assert option in ('us', 'usf', 'none', 'fft_in')\n",
    "        assert with_corr or with_eigen\n",
    "\n",
    "    def get_name(self):\n",
    "        selections = []\n",
    "        if self.option in ('us', 'usf', 'fft_in'):\n",
    "            selections.append(self.option)\n",
    "        if self.with_fft:\n",
    "            selections.append('fft')\n",
    "        if not self.with_corr:\n",
    "            selections.append('nocorr')\n",
    "        if not self.with_eigen:\n",
    "            selections.append('noeig')\n",
    "        if len(selections) > 0:\n",
    "            selection_str = '-' + '-'.join(selections)\n",
    "        else:\n",
    "            selection_str = ''\n",
    "        return 'freq-corr%s-%s-%s%s' % ('-phase' if self.use_phase else '', self.start_hz, self.end_hz, selection_str)\n",
    "\n",
    "    def apply_one(self, data, meta=None):\n",
    "        num_time_samples = data.shape[-1] if self.option != 'fft_in' else (data.shape[-1] - 1) * 2 # revert FFT shape change\n",
    "        if self.start_hz == 1 and self.end_hz is None:\n",
    "            freq_slice = Slice(self.start_hz, self.end_hz)\n",
    "        else:\n",
    "            # FFT range is from 0Hz to 101Hz\n",
    "            def calc_index(f):\n",
    "                return int((f / (meta.sampling_frequency/2.0)) * num_time_samples) if f is not None else num_time_samples\n",
    "            freq_slice = Slice(calc_index(self.start_hz), calc_index(self.end_hz))\n",
    "            # print data.shape, freq_slice.start, freq_slice.end\n",
    "            # import sys\n",
    "            # sys.exit(0)\n",
    "\n",
    "        data1 = data\n",
    "        if self.option != 'fft_in':\n",
    "            data1 = FFT().apply(data1)\n",
    "        data1 = freq_slice.apply(data1)\n",
    "        if self.use_phase:\n",
    "            data1 = np.angle(data1)\n",
    "        else:\n",
    "            data1 = Magnitude().apply(data1)\n",
    "            data1 = Log10().apply(data1)\n",
    "\n",
    "        data2 = data1\n",
    "        if self.option == 'usf':\n",
    "            data2 = UnitScaleFeat().apply(data2)\n",
    "        elif self.option == 'us':\n",
    "            data2 = UnitScale().apply(data2)\n",
    "\n",
    "        data2 = CorrelationMatrix().apply_one(data2)\n",
    "\n",
    "        if self.with_eigen:\n",
    "            w = Eigenvalues().apply_one(data2)\n",
    "\n",
    "        out = []\n",
    "        if self.with_corr:\n",
    "            data2 = upper_right_triangle(data2)\n",
    "            out.append(data2)\n",
    "        if self.with_eigen:\n",
    "            out.append(w)\n",
    "        if self.with_fft:\n",
    "            data1 = data1.ravel()\n",
    "            out.append(data1)\n",
    "        for d in out:\n",
    "            assert d.ndim == 1\n",
    "\n",
    "        return np.concatenate(out, axis=0)\n",
    "\n",
    "\n",
    "#FreqCorrelation(1, None, 'none').apply_one(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFT\n",
    "Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FFT:\n",
    "    \"\"\"\n",
    "    Apply Fast Fourier Transform to the last axis.\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return \"fft\"\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        axis = data.ndim - 1\n",
    "        return np.fft.rfft(data, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Magnitude abs\n",
    "Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Magnitude:\n",
    "    \"\"\"\n",
    "    Take magnitudes of Complex data\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return \"mag\"\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        return np.abs(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Binning\n",
    "Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FreqBinning(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Given spectral magnitude data, select a range of bins, and then choose a consolidation function\n",
    "    to use to calculate each bin. The sum can be used, or the mean, or the standard deviation.\n",
    "\n",
    "    NOTE(mike): Input for this transform must be from (FFT(), Magnitude())\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_ranges, func=None):\n",
    "        self.freq_ranges = freq_ranges\n",
    "        assert func is None or func in ('sum', 'mean', 'std')\n",
    "        self.func = func\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'fbin%s%s' % ('' if self.func is None else '-' + self.func, '-' + '-'.join([str(f) for f in self.freq_ranges]))\n",
    "\n",
    "    def apply_one(self, X):\n",
    "        num_channels = X.shape[0]\n",
    "        num_time_samples = (X.shape[-1] - 1) * 2 # revert FFT shape change\n",
    "\n",
    "        if self.func == 'mean':\n",
    "            func = np.mean\n",
    "        elif self.func == 'std':\n",
    "            func = np.std\n",
    "        else:\n",
    "            func = np.sum\n",
    "\n",
    "        # group into bins\n",
    "        def binned_freq(data, out):\n",
    "            prev = freq_ranges[0]\n",
    "            for i, cur in enumerate(freq_ranges[1:]):\n",
    "                prev_index = int(np.floor((prev / 399.6097561) * num_time_samples))\n",
    "                cur_index = int(np.floor((cur / 399.6097561399) * num_time_samples))\n",
    "                \n",
    "                out[i] = func(data[prev_index:cur_index])\n",
    "                prev = cur\n",
    "\n",
    "        freq_ranges = self.freq_ranges\n",
    "        out = np.empty((num_channels, len(freq_ranges) - 1,))\n",
    "        for ch in range(num_channels):\n",
    "            binned_freq(X[ch], out[ch])\n",
    "\n",
    "        result = out[0]\n",
    "        for i in range (1,len(out)):\n",
    "            result = np.concatenate([result, out[i]]) \n",
    "        \n",
    "        return result\n",
    "\n",
    "#FreqBinning([0.5, 2.25, 4, 5.5, 7, 9.5, 12, 21, 30, 39, 48],'mean').apply_one(Magnitude().apply(FFT().apply(channels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Log10\n",
    "Not a feature but used by other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Log10:\n",
    "\n",
    "    def get_name(self):\n",
    "        return \"log10\"\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        indices = np.where(data <= 0)\n",
    "        data[indices] = np.max(data)\n",
    "        data[indices] = (np.min(data) * 0.1)\n",
    "        return np.log10(data)\n",
    "\n",
    "#Log10().apply(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlattenChannels\n",
    "Not a feature but used by other features    \n",
    "Reshapes the data from (..., N_CHANNELS, N_FEATURES) to (..., N_CHANNELS * N_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FlattenChannels(object):\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'fch'\n",
    "\n",
    "    def apply(self, data, meta=None):\n",
    "        if data.ndim == 2:\n",
    "            return data.ravel()\n",
    "        elif data.ndim == 3:\n",
    "            s = data.shape\n",
    "            return data.reshape((s[0], np.product(s[1:])))\n",
    "        else:\n",
    "            raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIBSpectralEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PIBSpectralEntropy(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Similar to the calculations in SpectralEntropy transform, but instead power-in-band\n",
    "    is calculated over the given freq_ranges, finally Shannon entropy is calculated on that.\n",
    "    The output is a single entropy value per-channel.\n",
    "\n",
    "    NOTE(mike): Input for this transform must be from (FFT(), Magnitude())\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_ranges):\n",
    "        self.freq_ranges = freq_ranges\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'pib-spec-ent-%s' % '-'.join([str(f) for f in self.freq_ranges])\n",
    "\n",
    "    def apply_one(self, data, meta=None):\n",
    "        num_channels = data.shape[0]\n",
    "        num_time_samples = float((data.shape[-1] - 1) * 2) # revert FFT shape change\n",
    "\n",
    "        \n",
    "        def norm(X):\n",
    "            for i in range (len(X)):\n",
    "                for j in range (len(X[i])):\n",
    "                    X[i][j] /= X[i][j]+1e-12\n",
    "            return X\n",
    "\n",
    "\n",
    "        psd = data ** 2\n",
    "        psd = norm(psd)\n",
    "\n",
    "        # group into bins\n",
    "        def binned_psd(psd, out):\n",
    "            prev = freq_ranges[0]\n",
    "            for i, cur in enumerate(freq_ranges[1:]):\n",
    "                prev_index = int(np.floor((prev / 399.6097561) * num_time_samples))\n",
    "                cur_index = int(np.floor((cur / 399.6097561399) * num_time_samples))\n",
    "                out[i] = np.sum(psd[prev_index:cur_index])\n",
    "                prev = cur\n",
    "\n",
    "        freq_ranges = self.freq_ranges\n",
    "        out = np.empty((num_channels, len(freq_ranges) - 1,))\n",
    "        for ch in range(num_channels):\n",
    "            binned_psd(psd[ch], out[ch])\n",
    "\n",
    "        psd_per_bin = norm(out)\n",
    "\n",
    "        def entropy_per_channel(psd):\n",
    "            entropy_components = psd * np.log2(psd + 1e-12)\n",
    "            entropy = -np.sum(entropy_components) / np.log2(psd.shape[-1])\n",
    "            return entropy\n",
    "\n",
    "        out = np.empty((num_channels,))\n",
    "        for i, ch in enumerate(psd_per_bin):\n",
    "            out[i] = entropy_per_channel(ch)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "#PIBSpectralEntropy([0.25, 1, 1.75, 2.5, 3.25, 4, 5, 8.5, 12, 15.5, 19.5, 24]).apply_one(channels)\n",
    "#PIBSpectralEntropy([0.25, 2, 3.5, 6, 15, 24]).apply_one(channels)\n",
    "#PIBSpectralEntropy([0.25, 2, 3.5, 6, 15]).apply_one(channels)\n",
    "#PIBSpectralEntropy([0.25, 2, 3.5]).apply_one(channels)\n",
    "#PIBSpectralEntropy([6, 15, 24]).apply_one(channels)\n",
    "#PIBSpectralEntropy([2, 3.5, 6]).apply_one(channels)\n",
    "#PIBSpectralEntropy([3.5, 6, 15]).apply_one(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shannon Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ShannonEntropy(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Calculates Shannon entropy between the given frequency ranges.\n",
    "    e.g. The probability density function of FFT magnitude is calculated, then\n",
    "    given range [1,2,3], Shannon entropy is calculated between 1hz and 2hz, 2hz and 3hz\n",
    "    in this case giving 2 values per channel.\n",
    "\n",
    "    NOTE(mike): Input for this transform must be from (FFT(), Magnitude())\n",
    "    \"\"\"\n",
    "    def __init__(self, freq_ranges, flatten=True):\n",
    "        self.freq_ranges = freq_ranges\n",
    "        self.flatten = flatten\n",
    "\n",
    "    def get_name(self):\n",
    "        return 'spec-ent-%s%s' % ('-'.join([str(f) for f in self.freq_ranges]), '-nf' if not self.flatten else '')\n",
    "\n",
    "    def apply_one(self, fft_mag, meta=None):\n",
    "        num_time_samples = (fft_mag.shape[-1] - 1) * 2 # revert FFT shape change\n",
    "\n",
    "        X = fft_mag ** 2\n",
    "        for ch in X:\n",
    "            ch /= np.sum(ch + 1e-12)\n",
    "\n",
    "        psd = X # pdf\n",
    "\n",
    "        out = []\n",
    "\n",
    "        #[0,1,2] -> [[0,1], [1,2]]\n",
    "        for start_freq, end_freq in zip(self.freq_ranges[:-1], self.freq_ranges[1:]):\n",
    "            start_index = int(np.floor((start_freq / 399.6097561) * num_time_samples))\n",
    "            end_index = int(np.floor((end_freq / 399.6097561) * num_time_samples))\n",
    "            selected = psd[:, start_index:end_index]\n",
    "\n",
    "            entropies = - np.sum(selected * np.log2(selected + 1e-12), axis=selected.ndim-1) / np.log2(end_index - start_index)\n",
    "            if self.flatten:\n",
    "                out.append(entropies.ravel())\n",
    "            else:\n",
    "                out.append(entropies)\n",
    "\n",
    "        if self.flatten:\n",
    "            return np.concatenate(out)\n",
    "        else:\n",
    "            return np.asarray(out)\n",
    "\n",
    "#ShannonEntropy([1,2,3]).apply_one(Magnitude().apply(FFT().apply(channels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# higuchi-fractal-dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class HFD:\n",
    "    \n",
    "    def apply(self,epochs, Kmax = 2):\n",
    "        '''\n",
    "        Ported from https://www.mathworks.com/matlabcentral/fileexchange/30119-complete-higuchi-fractal-dimension-algorithm/content/hfd.m\n",
    "        by Salai Selvam V\n",
    "        '''\n",
    "        result = list()\n",
    "        \n",
    "        for epoch in epochs:\n",
    "            N = len(epoch)\n",
    "    \n",
    "            Lmk = np.zeros((Kmax,Kmax))\n",
    "    \n",
    "            #TODO: I think we can use the Katz code to refactor resampling the series\n",
    "            for k in range(1, Kmax+1):\n",
    "        \n",
    "                for m in range(1, k+1):\n",
    "               \n",
    "                    Lmki = 0\n",
    "            \n",
    "                    maxI = int((N-m)/k)\n",
    "            \n",
    "                    for i in range(1,maxI+1):\n",
    "                        Lmki = Lmki + np.abs(epoch[m+i*k-1]-epoch[m+(i-1)*k-1])\n",
    "             \n",
    "                    normFactor = (N-1)/(maxI*k)\n",
    "                    Lmk[m-1,k-1] = normFactor * Lmki\n",
    "    \n",
    "            Lk = np.zeros((Kmax, 1))\n",
    "    \n",
    "            #TODO: This is just a mean. Let's use np.mean instead?\n",
    "            for k in range(1, Kmax+1):\n",
    "                Lk[k-1,0] = np.nansum(Lmk[range(k),k-1])/k/k\n",
    "\n",
    "            lnLk = np.log(Lk) \n",
    "            lnk = np.log(np.divide(1., range(1, Kmax+1)))\n",
    "    \n",
    "            fit = np.polyfit(lnk,lnLk,1)  # Fit a line to the curve\n",
    "     \n",
    "            result.append(fit[0][0])   # Grab the slope. It is the Higuchi FD\n",
    "    \n",
    "        return np.array(result)\n",
    "    \n",
    "#HFD().apply(channels,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petrosian fractal dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PFD(ApplyManyTransform):\n",
    "    \"\"\"\n",
    "    Petrosian fractal dimension per-channel\n",
    "\n",
    "    Implementation derived from reading:\n",
    "    http://arxiv.org/pdf/0804.3361.pdf\n",
    "    F.S. Bao, D.Y.Lie,Y.Zhang,\"A new approach to automated epileptic diagnosis using EEG\n",
    "    and probabilistic neural network\",ICTAI'08, pp. 482-486, 2008.\n",
    "    \"\"\"\n",
    "    def get_name(self):\n",
    "        return 'pfd'\n",
    "\n",
    "    def pfd_for_ch(self, ch):\n",
    "        diff = np.diff(ch, n=1, axis=0)\n",
    "\n",
    "        asign = np.sign(diff)\n",
    "        sign_changes = ((np.roll(asign, 1) - asign) != 0).astype(int)\n",
    "        N_delta = np.count_nonzero(sign_changes)\n",
    "\n",
    "        n = len(ch)\n",
    "        log10n = np.log10(n)\n",
    "        return log10n / (log10n + np.log10(n / (n + 0.4 * N_delta)))\n",
    "\n",
    "    def apply_one(self, X, meta=None):\n",
    "        return np.asarray([self.pfd_for_ch(ch) for ch in X])\n",
    "\n",
    "#PFD().apply_one(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hurst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Hurst:\n",
    "    def apply(self,channels):\n",
    "        result = list()\n",
    "        for channel in channels:\n",
    "            x = np.array(channel)\n",
    "            x = x-x.mean()\n",
    "            z = np.cumsum(x)\n",
    "            r = np.array((np.maximum.accumulate(z) - np.minimum.accumulate(z))[1:])\n",
    "            s = pd.expanding_std(x)[1:]\n",
    "            s[np.where(s == 0)] = 1e-12\n",
    "            r += 1e-12\n",
    "            y_axis = np.log(r / s)\n",
    "            x_axis = np.log(np.arange(1, len(y_axis) + 1))\n",
    "            x_axis = np.vstack([x_axis, np.ones(len(x_axis))]).T\n",
    "            m, b = np.linalg.lstsq(x_axis, y_axis)[0]\n",
    "            result.append(m)\n",
    "            result.append(b)\n",
    "        return result\n",
    "\n",
    "#Hurst().apply(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HJORTH Fractal Dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hjorthFD(channels, Kmax=3):\n",
    "    result = list()\n",
    "    for X in channels:\n",
    "        L = []\n",
    "        x = []\n",
    "        N = len(X)\n",
    "        for k in range(1,Kmax):\n",
    "            Lk = []\n",
    "            \n",
    "            for m in range(k):\n",
    "                Lmk = 0\n",
    "                for i in range(1,int(math.floor((N-m)/k))):\n",
    "                    Lmk += np.abs(X[m+i*k] - X[m+i*k-k])\n",
    "                    \n",
    "                Lmk = Lmk*(N - 1)/math.floor((N - m) / k) / k\n",
    "                Lk.append(Lmk)\n",
    "                \n",
    "            L.append(np.log(np.nanmean(Lk)))   # Using the mean value in this window to compare similarity to other windows\n",
    "            x.append([np.log(float(1) / k), 1])\n",
    "\n",
    "        (p, r1, r2, s)= np.linalg.lstsq(x, L)  # Numpy least squares solution\n",
    "\t\n",
    "        result.append(p[0])\n",
    "    return result\n",
    "\n",
    "#hjorthFD(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# katz Fractal Dimention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def katzFD(channels):\n",
    "    result = list()\n",
    "    for data in channels:\n",
    "        n = len(data)-1\n",
    "        L = np.hypot(np.diff(data), 1).sum() # Sum of distances\n",
    "        d = np.hypot(data - data[0], np.arange(len(data))).max() # furthest distance from first point\n",
    "        a = np.log10(n) / (np.log10(d/L) + np.log10(n))\n",
    "        result.append(a)\n",
    "    return result\n",
    "\n",
    "#katzFD(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Spectral Density\n",
    " 1. Largest amplitude value in PSD.\n",
    " 2. Frequency of the largest peak.\n",
    " 3. Second largest amplitude value in PSD.\n",
    " 4. Frequency of the second largest peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def psd(channels):\n",
    "    result = list()\n",
    "    for channel in channels:\n",
    "        f, Pxx_den = signal.periodogram(list(channel))\n",
    "        f = f.tolist()\n",
    "        Pxx_den = Pxx_den.tolist()\n",
    "        max_one = max(Pxx_den)\n",
    "        freq_max_one = f[Pxx_den.index(max(Pxx_den))]\n",
    "        Pxx_den.pop(Pxx_den.index(max(Pxx_den)))\n",
    "        max_two = max(Pxx_den)\n",
    "        freq_max_two = f[Pxx_den.index(max(Pxx_den))]\n",
    "        result.append(max_one)\n",
    "        result.append(freq_max_one)\n",
    "        result.append(max_two)\n",
    "        result.append(freq_max_two)\n",
    "    return result\n",
    "\n",
    "#psd(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def moments(channels):  \n",
    "    result = list()\n",
    "    for data in channels:\n",
    "        skewness = stats.skew(data, axis=0, bias=True)\n",
    "        kurtosis = stats.kurtosis(data, axis=0, fisher=True, bias=True, nan_policy='propagate')\n",
    "        result.append(skewness)\n",
    "        result.append(kurtosis)\n",
    "    return result\n",
    "\n",
    "#moments(channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = list()\n",
    "test = list()\n",
    "for file in tqdm(glob.glob(\"dataset/Dog_5/*.mat\")):\n",
    "    if 'test' in file:\n",
    "        test.append(file)\n",
    "    else:\n",
    "        training.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features of the channels combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute_list = list()\n",
    "file_name = list()\n",
    "\n",
    "for files in tqdm(test): \n",
    "    file_read = si.loadmat(files)\n",
    "    #print file_read\n",
    "    keys = file_read.keys()\n",
    "    for i in range (len(keys)):\n",
    "        if 'test' in keys[i]:\n",
    "            channels = file_read[keys[i]][0][0][0]\n",
    "            file_name.append(keys[i])\n",
    "            feature_list = [\n",
    "                Correlation('usf').apply_one(channels),FreqCorrelation(1, None, 'none').apply_one(channels),\n",
    "                FreqBinning([0.5, 2.25, 4, 5.5, 7, 9.5, 12, 21, 30, 39, 48],'mean').apply_one(Magnitude().apply(FFT().apply(channels))),\n",
    "                PIBSpectralEntropy([0.25, 1, 1.75, 2.5, 3.25, 4, 5, 8.5, 12, 15.5, 19.5, 24]).apply_one(channels),\n",
    "                PIBSpectralEntropy([0.25, 2, 3.5, 6, 15, 24]).apply_one(channels),PIBSpectralEntropy([0.25, 2, 3.5, 6, 15]).apply_one(channels),\n",
    "                PIBSpectralEntropy([0.25, 2, 3.5]).apply_one(channels),PIBSpectralEntropy([6, 15, 24]).apply_one(channels),\n",
    "                PIBSpectralEntropy([2, 3.5, 6]).apply_one(channels), PIBSpectralEntropy([3.5, 6, 15]).apply_one(channels),\n",
    "                ShannonEntropy([1,2,3]).apply_one(Magnitude().apply(FFT().apply(channels))),\n",
    "                HFD().apply(channels,2), PFD().apply_one(channels), Hurst().apply(channels), hjorthFD(channels),\n",
    "                katzFD(channels), psd(channels), moments(channels)\n",
    "               ]\n",
    "            \n",
    "            result = feature_list[0]\n",
    "            for i in range (1,len(feature_list)):\n",
    "                result = np.concatenate([result, feature_list[i]]) \n",
    "            attribute_list.append(result)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "train_df['name'] = file_name\n",
    "train_df['attributes'] = attribute_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('d1features.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(len(train_df)):\n",
    "        feature_writer=list()\n",
    "        feature_writer.append(train_df['name'][i])\n",
    "        for val in train_df['attributes'][i]:\n",
    "            feature_writer.append(val)\n",
    "        \n",
    "        writer.writerow(feature_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
